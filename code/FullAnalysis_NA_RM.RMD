---
title: "Willamette Valley Gopher Analysis"
output: word_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

Shown below are the analysis steps for the Willamette Valley gopher project. The goals of this project are to explore the drivers of genetic variation across the Willamette Valley. The data set consists of 217 individuals sequenced at 6 loci using nuclear DNA. We identified ## null alleles that the use of Fst to explore genetic variation problematic. This document shows two broad analyses. First, I perform an analysis of individual genetic diversity using a discriminant analysis of the pricipal components of individual genetic heterogeneity. Secondly, I explored an analysis of the drivers of genetic variation across the Willamette Valley. We used the pairwise distances between individuals along the first # principal components describing individual genetic heterogeneity.   

```{r Environment, echo=FALSE,message=FALSE}

#Package installation code for packages instaled from github.

#if(!("devtools" %in% list.files(.libPaths()))) {
#  install.packages("devtools", repo = "http://cran.rstudio.com", dep = TRUE) 
#} # R tools are needed to install this package from github 
#devtools::install_github("wpeterman/ResistanceGA", build_vignettes = FALSE) # Tool to install the ResistanceGA package from github. 
# install.packages("ResistanceGA")

library(knitr)
library(ggplot2)
library(corrplot) #Used to make correlelograms
library(adegenet) # Used to conduct the DAPC analysis and convert the structure data.
library(readr) # Used to read in data
library(ecodist) # Used to measure the pairwise distances between individuals.
library(ResistanceGA) # Used to conduct the LMM analysis on the drivers of genetic diversity
library(MuMIn) #LMM Rsquared values
library(AICcmodavg) #Used constructing AIC and BIC tables
library(ade4) #Used to in variable reduction to principal components
library(factoextra) #Used to visualize principal coponents
#devtools::install_github('bcjaeger/r2glmm')
library(r2glmm)
#library(Momocs)
#library(pamctdp)
```

## Reading in data

The paths to these objects will need to be adjusted.

```{r Reading in data, echo = FALSE}
x<-read.structure("C:/Users/weldy/Projects/LUKE_GOPHER/DAPC/Gopher48pop217n6loci.str", n.ind = 217, n.loc = 6, onerowperind = FALSE, col.lab = 1, col.pop = 2, col.others = NULL, row.marknames = 1, NA.char = "999", pop = NULL, sep = NULL, ask = FALSE, quiet = FALSE)

#colnames(x$tab)
#Replacing NA values with Mean
x$tab <-tab(x, NA.method="mean",freq=TRUE)

x2<-read.structure("C:/Users/weldy/Projects/LUKE_GOPHER/DAPC/Gopher48pop217n6loci_NA_RM.str", n.ind = 217, n.loc = 6, onerowperind = FALSE, col.lab = 1, col.pop = 2, col.others = NULL, row.marknames = 1, NA.char = "000", pop = NULL, sep = NULL, ask = FALSE, quiet = FALSE)

```

## PCA

First I performed a PCA and assessed the significance of the PCA axes using the Kaiser-Guttman criterion and the broken stick model (Borcard et al. 2011; )

Borcard, D., Gillet, F. & Legendre, P. 2011. Numerical Ecology with R. Springer. 

"Kaiser-Guttman criterion - calculate the mean of all eigenvalues and interpret only axes with eigenvalues larger than this mean.
Broken stick model - randomly divides the stick of unit length into the same number of pieces as there are PCA axes and then sorts these pieces from the longest to the shortest. Repeats this procedure many times and averages the results of all permutations (analytical solution to this problem is also known). Broken stick model represents a null model and generates values of eigenvalues, which would occur at random. One may want to interpret only those PCA axes with eigenvalues larger than values generated by broken stick model (Fig. 3)."

Results are ambiguous according to the Broken stick model; however the Kaiser-Guttman criterion indicates that the first 11 axes are considered significant. 
```{r Dudi PCA}

set.seed(12)

pca<-dudi.pca(x$tab,center = TRUE, scale = FALSE, scann = FALSE, nf = 50) #30 axes

#str(pca)

evplot <- function(ev)
{
  # Broken stick model (MacArthur 1957)
  n <- length(ev)
  bsm <- data.frame(j=seq(1:n), p=0)
  bsm$p[1] <- 1/n
  
  for (i in 2:n) bsm$p[i] <- bsm$p[i-1] + (1/(n + 1 - i))
  bsm$p2 <- 100*bsm$p/n
  # Plot eigenvalues and % of variation for each axis
  op <- par(mfrow=c(2,1))
  barplot(ev, main="Eigenvalues", col="bisque", las=2)
  abline(h=mean(ev), col="red")
  legend("topright", "Average eigenvalue", lwd=1, col=2, bty="n")
  barplot(t(cbind(100*ev/sum(ev), bsm$p[n:1])), beside=TRUE, 
          main="% variation", col=c("bisque",2), las=2)
  legend("topright", c("% eigenvalue", "Broken stick model"), 
         pch=15, col=c("bisque",2), bty="n")
  par(op)
}
evplot(pca$eig)
```


## Optimizing Groups 

Given the exploratory nature of this analysis, I retained all 49 PCs when determining the optimal number of groups (K) to retain all available variability that might be useful in determining group clusters. The BIC curve had a sharp lower optim at 10, so I chose this as the optimal number of groups. However, there was a second small break point at k=4 suggesting the possibility of hierarchical clustering.

```{r Finding the optimal number of groups}
grp <- find.clusters(x$tab, method="kmeans",stat="BIC",n.pca = 49, n.clust = 10, n.iter=1e9,n.start=200, dudi=pca)

grp_k4 <- find.clusters(x$tab, method="kmeans",stat="BIC",n.pca = 30, n.clust = 4, n.iter=1e9,n.start=200, dudi=pca)
#find.clusters(x$tab, method="kmeans",stat="BIC",n.pca = 49, dudi=pca)
# grp_k2 <- find.clusters(x, method="kmeans",stat="BIC",n.pca = 30, n.clust = 2, n.iter=1e9,n.start=200, dudi=pca)
# grp_k3 <- find.clusters(x, method="kmeans",stat="BIC",n.pca = 30, n.clust = 3, n.iter=1e9,n.start=200, dudi=pca)
# grp_k4 <- find.clusters(x, method="kmeans",stat="BIC",n.pca = 30, n.clust = 4, n.iter=1e9,n.start=200, dudi=pca)
# grp_k10 <- find.clusters(x, method="kmeans",stat="BIC",n.pca = 30, n.clust = 10, n.iter=1e9,n.start=200, dudi=pca)
# setwd("C:/Users/weldy/Projects/LUKE_GOPHER/LMM")
# #write.csv(grp$grp, file="NA_REMOVED_10.csv")#To write a csv file of the grouping data remove the pound sign
# write.csv(grp_k2$grp, file="NA_REMOVED_6_k2.csv")#To write a csv file of the grouping data remove the pound sign
# write.csv(grp_k3$grp, file="NA_REMOVED_6_k3.csv")#To write a csv file of the grouping data remove the pound sign
# write.csv(grp_k4$grp, file="NA_REMOVED_4.csv")#To write a csv file of the grouping data remove the pound sign
# write.csv(grp_k10$grp, file="NA_REMOVED_6_k10.csv")#To write a csv file of the grouping data remove the pound sign
#grps<-as.data.frame(grp$grp)
```

```{r Plotting PCs}

par(mfcol=c(3,1))
s.class(pca$li, fac=grp$grp,xax=1, yax=2,
        col=transp(funky(15),.6),
        axesel=FALSE, cstar=0, cpoint=3)
add.scatter.eig(pca$eig[1:30],3,1,2, ratio=.3)
s.class(pca$li, fac=grp$grp,xax=2, yax=3,
        col=transp(funky(15),.6),
        axesel=FALSE, cstar=0, cpoint=3)
add.scatter.eig(pca$eig[1:30],3,2,3, ratio=.3)
s.class(pca$li, fac=grp$grp,xax=1, yax=3,
        col=transp(funky(15),.6),
        axesel=FALSE, cstar=0, cpoint=3)
add.scatter.eig(pca$eig[1:30],3,1,3, ratio=.3)

```
Axes 1 and 2
```{r}

fviz_pca_biplot(pca, repel = FALSE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969",  # Individuals color
                axes = c(1,2)
)

```
Axes 2 and 3
```{r}

fviz_pca_biplot(pca, repel = FALSE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969",  # Individuals color
                axes = c(2,3)
)

```
Axes 1 and 3
```{r}
fviz_pca_biplot(pca, repel = FALSE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969",  # Individuals color
                axes = c(1,3)
)

```






```{r Group Allignment Table}
table.value(table(pop(x), grp$grp), col.lab=paste("inf", 1:10),
            row.lab=paste("ori", 1:48))


```

## DAPC

I performed the DAPC using the optimized groups and PCs estimated earlier (30 PCAs, and 10 discriminant components). The plots below show the variation along DA 1 and DA2, DA2 and DA3, and DA1 and DA 3.
```{r DAPC}
dapc1 <- dapc(x, grp$grp, n.pca=49, n.da=10,center = TRUE, scale = FALSE, dudi=pca) #retained 50 PCs and 10
getwd()
#jpeg("DAPC_NO_NA.jpeg", width = 7, height = 7, units = 'in', res = 600)
par(mfrow=c(3,1))
scatter(dapc1, xax=1, yax=2, col=funky(10),grp=dapc1$grp,cell=0,cstar=0 ,label.inds = TRUE)
scatter(dapc1, xax=2, yax=3, col=funky(10),grp=dapc1$grp,cell=0,cstar=0)
scatter(dapc1, xax=1, yax=3, col=funky(10),grp=dapc1$grp,cell=0,cstar=0)
#dev.off()
#funky(10)
#funky(4)
#ggsave("NEW_ALL_N_1_NO_NA.jpeg", plot = p1,  scale = 1, width = 7, height = 9.5, units = "in", dpi = 600)


```

Reseting graphical parameters
```{r,message=FALSE}
par(mfcol=c(1,1))
```

Group membership probabilities. Red indicates high probability of classification to the clusters identified using the find clusters function above (indicated by blue crosses).
```{r, fig.width=8, fig.height=11}

par(mfcol=c(3,2))
assignplot(dapc1,subset=1:50)
assignplot(dapc1, subset=51:100)
assignplot(dapc1, subset=101:150)
assignplot(dapc1, subset=151:200)
assignplot(dapc1, subset=200:217)
```

```{r ,echo=FALSE,message=FALSE}
par(mfcol=c(1,1))
```

Examining admixture with the optimal clusters. We have good classification to groups here. Lots of improvement over the a priori clusters.
```{r, fig.width=8, fig.height=11}
par(mfcol=c(3,2))
compoplot(dapc1, subset=1:40, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))
compoplot(dapc1, subset=41:80, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))
compoplot(dapc1, subset=81:120, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))
compoplot(dapc1, subset=121:160, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))
compoplot(dapc1, subset=161:200, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))
compoplot(dapc1, subset=201:217, posi="bottomright",
          txt.leg=paste("Cluster", 1:10), show.lab=TRUE, 
          ncol=2, xlab="individuals", col=funky(10))

# compoplot(dapc1,
#           txt.leg=paste("Cluster", 1:11), show.lab=FALSE, 
#           xlab="individuals", col=funky(15), n.col=1,posi="right")
# 
# leg<-as.factor(c('Cluster 1','Cluster 2','Cluster 3','Cluster 4','Cluster 5','Cluster 6','Cluster 7','Cluster 8',
#                  'Cluster 9','Cluster 10','Cluster 11', 'Cluster 12', 'Cluster 13', 'Cluster 14', 'Cluster 15'))
# jpeg("Plot3_NO_NA2.jpeg", width = 7, height = 5, units = 'in', res = 600)
# par(mar=c(4, 4, 2, 5.5), xpd=TRUE)
# barplot(t(dapc1$posterior),  width = 1,col = funky(10),names.arg = NULL,axisnames = FALSE,
#         ylab = "membership probability", xlab = "Individuals",border =NA, bty='L')
# legend(x=260, y=0.93, legend = leg, 
#        fill = funky(15))
# library(ggplot2)
# head(PCA_df)
# ggplot(data=PCA_df, aes(x=dose, y=len)) +
#   geom_bar(stat="identity")
# PCA_df
# PCA_dfc
# dev.off()
# ?write.csv
# 
# 
# write.csv(t(dapc1$posterior),file="compoplot3.csv")
# write.csv(dapc1$posterior,file="compoplot3.csv")
```


## Linear Mixed Effects Models for Individual-based Genetic Distances
Here I used linear mixe effects models to evaluate the effect of 5 landscape variables on the connectivity of Willamette Valley gopher populations. The response variable was the individual-based pairwise genetic distance, and the explanatory variables were resistance surfaces estimated in Circuitscape. We estimated the individual-based pairwise genetic distance as the distance between individual coordinates along the first 11 prinicipal component axes. We used 11 principal axes, because these were identified as the significant axes by the Kaiser-Guttman criterion. We measure the distance between individuals along these 11 axes using the distance function in the ecodist R package. We used a MLPE parameterization of the LMM models as established by Clark et al. 2002.

```{r Individual PC Coordines}

PCA_DUDI<-pca$li
a<-as.matrix(dist(PCA_DUDI[,1:11], method = "euclidean")) # Using the first 11 PCs

```

Estimating the euclidean distances between individuals
```{r Euclidean Distances of Individuals}
XY<-read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/SPATIAL_DATA.csv") #Euclidean Distances of Individuals
XY<-as.matrix(dist(XY[3:4], method = "euclidean"))

```

Mantel tests for distance. Pval1 indicates a one-tailed p-value (Null: r<=0), pval2 indicates a one-tailed p-value (Null:r>=0), and Pval3 indicates a two-tailed p-value (Null: r=0)
```{r Mantel test of Euclidean Distances}
mantel(lower(a) ~ lower(XY))

```

```{r Plot of Mantel test of Euclidean Distances}

z.mgram <- mgram(lower(a), lower(XY), nclass= 40)
plot(z.mgram,  pval = 0.05, xlab = "Distance", ylab = "Mantel r")
```

# Exploring the drivers of genetic variation with linear mixed effects models.

```{r, echo=FALSE}
id <- To.From.ID(217) #Calculating an index for the individual pairwise distances using a ResistancesGA function

time<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/Time.csv"))
colnames(time) <- NULL 
dist<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/dist.csv"))
colnames(dist) <- NULL 
river<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/river.csv"))
colnames(river) <- NULL 
slope<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/slope.csv"))
colnames(slope) <- NULL 
river_plus_slope<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/river_plus_slope_resistances.csv"))
colnames(river_plus_slope) <- NULL 
river_plus_slope_beta<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/river_slope_betascale_resistances.csv"))
colnames(river_plus_slope_beta) <- NULL 

slope3<-as.matrix(read.csv("C:/Users/weldy/Projects/LUKE_GOPHER/LMM/SlopeDeg500m217exp3_squared_resistances.csv"))
colnames(slope3) <- NULL 
```



```{r,echo=FALSE,message=FALSE}
range01 <- function(x){(x-min(x))/(max(x)-min(x))}
df <- data.frame(y = lower(a),
                 time = lower(time),
                 dist = lower(dist),
                 river = lower(river),
                 slope =lower(slope),
                 river_plus_slope= lower(river_plus_slope),
                 river_plus_slope_beta= lower(river_plus_slope_beta),
                 slope3 =lower(slope3),
                 pop = id$pop1,
                 otherpop = id$pop2)

summary(df)
# library(mgcv)
# fixDependence(river,slope,strict=TRUE)
# ?fixDependence
```

```{r}
correl = cor(df[,1:7],method='pearson')
corrplot(correl, method = "circle",addCoef.col = "grey")
```

```{r}
# Fit a prioi  models
Cand.models <- list( )
Cand.models[[1]] <-mlpe_rga(formula = y ~ (1 | pop),
                            data = df)
Cand.models[[2]] <-mlpe_rga(formula = y ~ dist + (1 | pop),
                            data = df)
Cand.models[[3]] <-mlpe_rga(formula = y ~ river + (1 | pop),
                            data = df)
Cand.models[[4]] <-mlpe_rga(formula = y ~ slope + (1 | pop),
                            data = df)
Cand.models[[5]] <-mlpe_rga(formula = y ~ river + slope + (1 | pop),
                            data = df)
Cand.models[[6]] <-mlpe_rga(formula = y ~ river_plus_slope + (1 | pop),
                            data = df)

```


We used 4 metrics to assist in selecting the best model. First we estimated both AIC and BIC, which Row et al. (2017) demonstrated as more reliable indicators of 'true models' when used in individual based landscape genetics simulation studies. We also considered marginal R squared values, using the method established by Nakagawa and Schielzeth (2014) for linear mixed effects models, to estimate how well each model described our data. Lastly we estimated the 95% confidence limits for each landscape variables beta parameter. 
```{r}

names<-rbind('null','dist','river','slope','river plus slope','river_slope')

?aictab
AIC<-aictab(Cand.models, sort = TRUE,
            second.ord = FALSE, modnames=names)
BIC<-bictab(Cand.models, sort = TRUE,
            second.ord = FALSE, modnames=names)

rsqvaltab<-as.data.frame(rbind(r.squaredGLMM(Cand.models[[5]]),
                               r.squaredGLMM(Cand.models[[3]]),
                               r.squaredGLMM(Cand.models[[2]]),
                               r.squaredGLMM(Cand.models[[6]]),
                               r.squaredGLMM(Cand.models[[4]]),
                               
                               r.squaredGLMM(Cand.models[[1]])
))

MODSELTAB<-as.data.frame(cbind(AIC$Modnames,AIC$K,AIC$AIC,AIC$Delta_AIC,BIC$BIC,BIC$Delta_BIC,AIC$LL,rsqvaltab))
names(MODSELTAB)<-list('Modnames','K','AIC','Delta_AIC','BIC','Delta_BIC','LL','Marginal Rsq', 'Conditional Rsq')
kable(MODSELTAB)
```



```{r}
confint1<-confint(Cand.models[[1]], level = 0.95)
confint2<-confint(Cand.models[[2]], level = 0.95)
confint3<-confint(Cand.models[[3]], level = 0.95)
confint4<-confint(Cand.models[[4]], level = 0.95)
confint5<-confint(Cand.models[[5]], level = 0.95)
confint6<-confint(Cand.models[[6]], level = 0.95)
confint<-as.data.frame(rbind(c(confint1[3,1:2], Cand.models[[1]]@beta[[1]]),
                             c(confint2[4,1:2], Cand.models[[2]]@beta[[2]]),
                             c(confint3[4,1:2], Cand.models[[3]]@beta[[2]]),
                             c(confint4[4,1:2], Cand.models[[4]]@beta[[2]]),
                             c(confint5[4,1:2], Cand.models[[5]]@beta[[2]]),
                             c(confint5[5,1:2], Cand.models[[5]]@beta[[3]]),
                             c(confint6[4,1:2],Cand.models[[6]]@beta[[2]])
))
confint$model<-rbind('null','dist','river','slope','river plus slope','river plus slope','river_slope')
confint$betacoef<-rbind('null','dist','river','slope','river','slope','river_slope')
names(confint)<-c('lcl','ucl','beta','model','beta coef')
kable(confint)

```

According to AIC and BIC, we found strong support that rivers affect the landscape genetics of gophers in the Willamette Valley and slight support for an additional marginal affect of slope. However, evidence provided by the marginal R squared values indicates that slope was better at describing the data. This suggests that the top ranking model with a fixed effect for rivers and a fixed effect for slope may be a better model for predicting to future data; however, the scale of the slope parameter (high maximum values) may disproportionately affect the parameters ability to capture variation within our dataset. 

```{r ,echo=FALSE,message=FALSE}
par(mfcol=c(1,1))
```



<!-- ```{r} -->
  <!-- library(maps) -->
  <!-- library(mapdata) -->
  <!-- ?map -->
  <!-- map(database= 'county',region= 'Oregon', col='gray90', ylim=c(30,50),xlim=c(-123,-122),fill=TRUE) -->
  <!-- map(database= 'county',region= 'Oregon', col='gray90', fill=TRUE) -->
  <!-- xlim=c(-141,-53), ylim=c(40,85), -->
  <!-- ``` -->
  
  
  ## Model Fit Diagnostics
```{r}
df$resnull = residuals(Cand.models[[1]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[1]]), df$resnull)
plot(df$y, df$resnull)
qqnorm(df$resnull)
hist(df$y)
```


```{r,echo=FALSE,message=FALSE}
par(mfcol=c(1,1))
```

Model Diagnostics for distance model.
```{r}
df$resdist = residuals(Cand.models[[2]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[2]]), df$resdist)
plot(df$y, df$resdist)
qqnorm(df$resdist)
plot(df$dist, df$resdist, main="Residuals vs. Distance")
```

```{r,echo=FALSE,message=FALSE}
par(mfcol=c(1,1))
```

Model Diagnostics for river model.
```{r}
df$resriver = residuals(Cand.models[[3]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[3]]), df$resriver)
plot(df$y, df$resriver)
qqnorm(df$resriver)
plot(df$river, df$resriver, main="Residuals vs. River")
```

```{r,echo=FALSE,message=FALSE}
par(mfcol=c(1,1))
```

Model Diagnostics for slope model.
```{r}
df$resslope = residuals(Cand.models[[4]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[4]]), df$resslope)
plot(df$y, df$resslope)
qqnorm(df$resslope)
plot(df$slope, df$resslope, main="Residuals vs. Slope")
```


```{r}
df$resslope = residuals(Cand.models[[5]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[5]]), df$resslope)
plot(df$y, df$resslope)
qqnorm(df$resslope)
plot(df$slope, df$resslope, main="Residuals vs. Slope")
```

```{r}
df$resslope = residuals(Cand.models[[6]], type = "pearson")
par(mfcol=c(2,2))
plot(fitted(Cand.models[[6]]), df$resslope)
plot(df$y, df$resslope)
qqnorm(df$resslope)
plot(df$slope, df$resslope, main="Residuals vs. Slope")
```